# DataLeakage Detection README

## Table of Contents

1. [Preliminaries](#preliminaries)
2. [Modules/Architecture](#modules-architecture)
3. [Module Details](#module-details)
4. [Execution Instructions](#execution-instructions)
5. [Notes](#notes)

---
## Prerequisites:
After downloading the folder locally, install the required packages
listed in the [requirements.txt](requirements.txt) file, into the project folder.

You may use the PyCharm IDE, or the command: `pip install -r requirements.txt`

The recommended python version is 3.9

# Modules
The Permission Misuse Detection Method use the following modules:

![Modules Diagram](diagram.png)

In the [config.ini](configuration/config.ini) file, you need to change the following values:

To execute the parsers run the following command:

 `python run.py configuration/config.ini PMD_modules`

The config_parser.ini file is located in the AF_Transfer/config folder and composed of the following modules:

**The LogParser module**

imports the cloud trail logs from AWS and creates ini files for the LogStreamParser module.
This module is a relevant to all the use cases and (e.g. Data Lackeg, permission misused, etc.),
and can be found in the preprocessing folder. This module can be modified and contains the following parameters:

- `application` - the name of the application (e.g. "VOD", "airline").
- `output_path` - Path to save the csvs on.
- `current_run` - the current run name, used for specifying the output folder.
- `from_date` - start date for the data collection. format: 'YYYY-MM-DD HH:MM:SS'
- `to_date` - end date for the data collection. format: 'YYYY-MM-DD HH:MM:SS'
- `time_split` - the time window for each file in hours.
- `type` - the type of the module. can be "importer" or "parser".
- `PATH_TO_PYENV` - path to the python environment.
- `user` - the module name, "AF" in the current case.
- 
**The LogStreamParser module**

parses the csv files generated by the LogParser module and creates csv files for the ApplicationFlowsDetection module.
This module is a relevant to all the use cases and (e.g. Data Lackeg, permission misused, etc.),
and can be found in the preprocessing folder. This module can be modified and contains the following parameters:

- `application` - the name of the application (e.g. "VOD", "airline").
- `output_path` - Path to save the csvs on.
- `current_run` - the current run name, used for specifying the output folder.
- `lambdas_csv_name` - the name of the csv file that contains the lambdas' names, "Lambdas_Logstream.csv".
- `access_api_csv_name` - the name of the csv file that contains the access api events, "storage_API_calls.csv.
- `permission_change_csv_name` - the name of the csv file that contains the permission change events, "permission_change.csv".
- `events_dic_name` - the name of the dictionary that contains the events' names, "cloudWatch_events".
- `lambda_invoke` - whether to include the lambda invoke events or not.
- `access_api` - whether to include the access api events or not.
- `permission_change` - whether to include the permission change events or not.
- `all_logs_in_same_dir` - whether all the logs are in the same directory or not.
- `jsons_path` - path to the json files, need to start with "data/output/AWSImporter/" and then the current_run.
- `invokes_json_path` - path to the invokes json files, "aws-cloudtrail-logs-Ecommerce_logs.json".
- `API_json_path` - path to the API json files, "aws-cloudtrail-logs-Ecommerce_logs.json".
- `permission_json_path` - path to the permission json files, "aws-cloudtrail-logs-serveressairline-PermissionChanges_logs.json".
- `lambdas_event_json_path` - path to the lambdas event json files, "lambdas_event.json".

**The XrayParser module**

that import the Xray debugging logs from AWS and creates csv files with the combined csv files from the cloudwatch logs.
This module can be modified and contains the following parameters:

- `output_path` - Path to save the csvs on.
- `start_time` - start date for the data collection. format: 'YYYY-MM-DD HH:MM:SS'
- `end_time` - end date for the data collection. format: 'YYYY-MM-DD HH:MM:SS'
- `lambdaLS_file_path` - path to the lambdas event csv files.
- `subdicts_depth` - the depth of the directory.
- `results_dict` - Path to the csv files worked with(could be the same as `output_path`)

After configuring the values in the config file, run the file [run.py](run.py) (for anomalous and  benign data, as needed)
with the config.ini file path as a parameter/argument.

## Model Prediction Module
In the [config.json](config.json) file, you need to configure the files you want to make the training on
(by default, the files and setting are set to run with the existing files):  
- `XRAY_logs_path` - the files that the model will use as train and validation sets.
- `XRAY_logs_path_test` - the files that the model will use as test set. 
- `event_cols_ls` - what columns are being ,default:["lambda_name", "name", "operation_aws"].
- `lstm_type` - the type of LSTM model created, could be bidirectional, GRU or stacked, default is standard.
- `n_for_ngrams` - number of series size for ngram.  default is 16.
- `threshold_validation_FP_quantile` - what is the quantile for the threshold, default is 0.999.
- `THR_according_val_set` - what is the threshold for the validation set default is 0.8.
- `epochs`- how many epochs will take place, default is 15.
- `batch_size` - what is the batch size, default is 16.
- `lr` - what is the learning rate for the model, default is 0.0003.
- `decady` - bool, flag to enable learning decay,  default is true.

After configuring the values in the configuration file, run the file [main.py](main.py) file.
